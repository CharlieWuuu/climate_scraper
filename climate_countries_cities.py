import aiohttp
import asyncio
import csv
import re
import os
from bs4 import BeautifulSoup

# è¨­å®šçˆ¬å–çš„ç¶²å€s
base_url = "https://en.climate-data.org/a/a/a/a-{}/"
output_file = "climate_countries_cities.csv"
MAX_CONCURRENT_REQUESTS = 20  # é™åˆ¶åŒæ™‚è«‹æ±‚æ•¸é‡
BATCH_SIZE = 100  # æ¯æ¬¡æœ€å¤šæ–°å¢ 30 ç­†è³‡æ–™
months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]

# **è®€å– CSVï¼Œæ‰¾å‡ºä¸å®Œæ•´çš„è³‡æ–™ä¸¦ç¢ºä¿ ID é€£çºŒ**
def get_incomplete_and_missing_ids():
    existing_ids = set()
    incomplete_ids = []
    existing_data = []

    if os.path.exists(output_file):
        with open(output_file, mode="r", encoding="utf-8") as infile:
            reader = csv.reader(infile)
            headers = next(reader)  # è®€å–æ¨™é¡Œ
            for row in reader:
                if not row:
                    continue

                city_id = int(row[0])
                country, region, city = row[1:4]
                climate_type, avg_temp_c, annual_rain_mm = row[4:7]

                existing_ids.add(city_id)

                # **å¦‚æœè³‡æ–™ä¸å®Œæ•´ï¼ŒåŠ å…¥é‡æ–°çˆ¬å–æ¸…å–®**
                if country != "-" and city == "-":
                    incomplete_ids.append(city_id)
                else:
                    existing_data.append(row)

    # **ç¢ºä¿ ID é€£çºŒ**
    max_id = max(existing_ids) if existing_ids else 0
    missing_ids = [i for i in range(1, max_id + BATCH_SIZE + 1) if i not in existing_ids]

    print(f"ğŸ” æ‰¾åˆ° {len(incomplete_ids)} ç­†ä¸å®Œæ•´çš„è³‡æ–™")
    print(f"ğŸ“Œ éœ€è¦æ–°å¢ {len(missing_ids[:BATCH_SIZE])} ç­†æ–°è³‡æ–™")

    return incomplete_ids, missing_ids[:BATCH_SIZE], headers, existing_data

# **çˆ¬å–å–®å€‹åŸå¸‚é é¢**
async def fetch_city(session, city_id):
    url = base_url.format(city_id)
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        async with session.get(url, headers=headers, timeout=10) as response:
            if response.status == 404:
                print(f"ID {city_id}: 404 Not Foundï¼Œå­˜ç©ºå€¼")
                return [city_id, "-", "-", "-", "-", "-", "-", "-", "-", "-", *["-"] * 12 * 7]

            html = await response.text()
            soup = BeautifulSoup(html, "html.parser")

            # **è§£æåœ°ç†è³‡è¨Š**
            breadcrumbs = soup.select("ol[itemtype='http://schema.org/BreadcrumbList'] li span[itemprop='name']")
            location_hierarchy = [bc.text.strip() for bc in breadcrumbs[1:]] if breadcrumbs else []

            country, region, city = "-", "-", "-"
            if len(location_hierarchy) == 3:
                country, region, city = location_hierarchy
            elif len(location_hierarchy) == 2:
                country, region, city = location_hierarchy[0], "-", location_hierarchy[1]
            elif len(location_hierarchy) == 1:
                country, region, city = location_hierarchy[0], "-", "-"

            print(f"ID {city_id}: {country}, {region}, {city} âœ…")

            # **è§£ææ°£å€™æ•¸æ“š**
            article = soup.select_one("#article")
            if article:
                text = article.get_text()

                # **æŠ“å– KÃ¶ppen-Geiger æ°£å€™åˆ†é¡**
                climate_match = re.search(r"\b(Af|Am|As|Aw|BWh|BWk|BSh|BSk|Cfa|Cfb|Cfc|Cwa|Cwb|Cwc|Csa|Csb|Csc|Dfa|Dfb|Dfc|Dfd|Dwa|Dwb|Dwc|Dwd|Dsa|Dsb|Dsc|Dsd|ET|EF)\b", text)
                climate_type = climate_match.group(1).strip() if climate_match else "-"

                # **æŠ“å–å¹´å‡æº«ã€é™é›¨é‡**
                temp_match = re.search(r"temperature.*? ([\d.]+) Â°C", text)
                avg_temp_c = temp_match.group(1) if temp_match else "-"

                rain_match = re.search(r"rainfall.*? ([\d.]+) mm", text)
                annual_rain_mm = rain_match.group(1) if rain_match else "-"

                # **æŠ“å–æ‰€å±¬åŠçƒã€å¤å­£æœˆä»½ã€æœ€ä½³æ—…éŠæ™‚é–“**
                hemisphere_match = re.search(r"([Nn]orthern|[Ss]outhern) [Hh]emisphere", text)
                hemisphere = hemisphere_match.group(1).capitalize() if hemisphere_match else "-"

                summer_match = re.search(r"Summer.*? ([A-Za-z, ]+)", text)
                summer_months = summer_match.group(1).strip() if summer_match else "-"

                visit_match = re.search(r"best time to visit is ([A-Za-z, ]+)", text)
                best_visit_time = visit_match.group(1).strip() if visit_match else "-"

                # **è§£æ monthly weather data**
                weather_table = soup.select_one("#weather_table tbody")
                monthly_data = {}
                if weather_table:
                    rows = weather_table.find_all("tr")
                    for row in rows:
                        cols = row.find_all("td")
                        if cols:
                            key = cols[0].text.strip()
                            values = [re.sub(r'[^\d.-]', '', col.text.strip().splitlines()[0]) for col in cols[1:]]
                            monthly_data[key] = values

                # **è½‰æ› monthly_data**
                weather_data_expanded = []
                for key in ["Avg. Temperature Â°C (Â°F)", "Min. Temperature Â°C (Â°F)", "Max. Temperature Â°C (Â°F)",
                            "Precipitation / Rainfall mm (in)", "Humidity(%)", "Rainy days (d)", "avg. Sun hours (hours)"]:
                    values = monthly_data.get(key, ["-"] * 12)
                    weather_data_expanded.extend(values)

                return [city_id, country, region, city, climate_type, avg_temp_c,
                        annual_rain_mm, hemisphere, summer_months, best_visit_time, *weather_data_expanded]

            print(f"ID {city_id}: âŒ æ‰¾ä¸åˆ°æ•¸æ“šï¼Œå­˜ç©ºå€¼")
            return [city_id, "-", "-", "-", "-", "-", "-", "-", "-", "-", *["-"] * 12 * 7]

    except Exception as e:
        print(f"ID {city_id}: âŒ é€£ç·šéŒ¯èª¤ - {e}")
        return [city_id, "-", "-", "-", "-", "-", "-", "-", "-", "-", *["-"] * 12 * 7]

# **åŸ·è¡Œçˆ¬å–**
async def scrape_cities():
    incomplete_ids, new_ids, headers, existing_data = get_incomplete_and_missing_ids()

    ids_to_fetch = incomplete_ids + new_ids  # **åˆä½µéœ€è¦é‡æ–°çˆ¬å–å’Œæ–°å¢çš„ ID**
    if not ids_to_fetch:
        print("âœ… æ²’æœ‰éœ€è¦é‡æ–°æŠ“å–çš„è³‡æ–™")
        return

    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    async with aiohttp.ClientSession() as session:
        async def limited_fetch(city_id):
            async with semaphore:
                return await fetch_city(session, city_id)

        tasks = [limited_fetch(city_id) for city_id in ids_to_fetch]
        new_results = await asyncio.gather(*tasks)

    # **åˆä½µèˆŠè³‡æ–™èˆ‡æ–°è³‡æ–™ï¼Œä¸¦æŒ‰ç…§ ID æ’åº**
    combined_data = existing_data + new_results
    combined_data.sort(key=lambda x: int(x[0]))  # **ç¢ºä¿ ID é€£çºŒæ’åº**

    # **è¦†å¯« CSV**
    with open(output_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(headers)
        writer.writerows(combined_data)

    print(f"âœ… çˆ¬å–å®Œæˆï¼Œè³‡æ–™å·²æ›´æ–°ä¸¦é‡æ–°æ’åºè‡³ {output_file}")

# **åŸ·è¡Œçˆ¬èŸ²**
asyncio.run(scrape_cities())
