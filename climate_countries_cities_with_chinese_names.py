import aiohttp
import asyncio
import csv
import os
from bs4 import BeautifulSoup

# NAER å¤–åœ‹åœ°åè­¯åæŸ¥è©¢ API
naer_url = "https://terms.naer.edu.tw/search/"

# è¨­å®šè¼¸å…¥ / è¼¸å‡º CSV æª”æ¡ˆ
input_file = "climate_countries_cities.csv"
output_file = "climate_countries_cities_with_chinese_names.csv"

# è¨­å®šæœ€å¤§åŒæ™‚è«‹æ±‚æ•¸é‡ & æŸ¥è©¢ç­†æ•¸
MAX_CONCURRENT_REQUESTS = 20  # æ§åˆ¶ä¸¦ç™¼æ•¸é‡ï¼Œé¿å… API éè¼‰
BATCH_SIZE = 100  # æ¯æ¬¡æœ€å¤šæŸ¥è©¢ 100 ç­†ï¼Œç„¶å¾ŒçµæŸ

# è®€å–è¼¸å…¥æª”æ¡ˆï¼Œç¢ºä¿æ‰€æœ‰ ID é€£çºŒ
def get_all_ids_from_input():
    all_ids = set()
    city_data = {}  # å­˜æ”¾å®Œæ•´è³‡æ–™
    if os.path.exists(input_file):
        with open(input_file, mode="r", encoding="utf-8") as file:
            reader = csv.reader(file)
            next(reader, None)  # è·³éæ¨™é¡Œ
            for row in reader:
                city_id = int(row[0])
                all_ids.add(city_id)
                city_data[city_id] = row  # å­˜ ID -> row è³‡æ–™
    return all_ids, city_data

# è®€å–å·²æŸ¥è©¢çš„åŸå¸‚ä¸­æ–‡åç¨±
def get_existing_data():
    existing_data = {}  # å­˜æ”¾å·²æŸ¥è©¢éçš„è³‡æ–™
    if os.path.exists(output_file):
        with open(output_file, mode="r", encoding="utf-8") as file:
            reader = csv.reader(file)
            next(reader, None)  # è·³éæ¨™é¡Œ
            for row in reader:
                city_id = int(row[0])
                existing_data[city_id] = row  # å­˜æ•´è¡Œ
    return existing_data

# æŸ¥è©¢å–®å€‹åŸå¸‚çš„ä¸­æ–‡åç¨±
async def fetch_chinese_name(session, city):
    params = {
        "query_term": city,
        "query_field": "title",
        "query_op": "",
        "match_type": "phrase",
        "filter_bool": "and",
        "filter_term": "'å¤–åœ‹åœ°åè­¯å'",
        "filter_field": "subcategory_1.raw",
        "filter_op": "term",
        "tabaction": "browse"
    }
    headers = {"User-Agent": "MyProject/1.0 (charliewu500@gmail.com)"}

    try:
        async with session.get(naer_url, params=params, headers=headers, timeout=10) as response:
            if response.status != 200:
                print(f"âŒ {city}: HTTP {response.status} éŒ¯èª¤")
                return "N/A"

            html = await response.text()
            soup = BeautifulSoup(html, "html.parser")

            # æŸ¥æ‰¾åŒ…å«ä¸­æ–‡åç¨±çš„ <div>
            result_div = soup.find("div", class_="td", attrs={"aria-label": "ä¸­æ–‡è©å½™"})
            if result_div and result_div.a:
                chinese_name = result_div.a.text.strip()
                print(f"âœ… {city} â†’ {chinese_name}")
                return chinese_name
            else:
                print(f"âŒ {city}: ç„¡å°æ‡‰çµæœ")
                return "Not Found"
    except Exception as e:
        print(f"âŒ {city}: é€£ç·šéŒ¯èª¤ - {e}")
        return "Error"

# æ‰¹é‡æŸ¥è©¢ä¸­æ–‡åç¨±ï¼ˆé™åˆ¶æ¯æ¬¡ 100 ç­†ï¼‰
async def fetch_all_chinese_names(city_list):
    results = []
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)  # é™åˆ¶åŒæ™‚è«‹æ±‚æ•¸é‡

    async with aiohttp.ClientSession() as session:
        async def limited_fetch(city_id, country, region, city):
            async with semaphore:
                chinese_name = await fetch_chinese_name(session, city)
                results.append([city_id, country, region, city, chinese_name])

        print(f"ğŸ” æ­£åœ¨æŸ¥è©¢ {len(city_list)} ç­†åŸå¸‚çš„ä¸­æ–‡åç¨±...")
        tasks = [limited_fetch(city_id, country, region, city) for city_id, country, region, city in city_list]
        await asyncio.gather(*tasks)

    return results

# è®€å– CSV ä¸¦åŸ·è¡ŒéåŒæ­¥æŸ¥è©¢
async def main():
    all_ids, city_data = get_all_ids_from_input()  # è®€å–è¼¸å…¥æª”æ¡ˆçš„æ‰€æœ‰ ID
    existing_data = get_existing_data()  # è®€å–å·²æŸ¥è©¢çš„ ID
    city_list = []
    all_results = []  # å­˜æ”¾æœ€çµ‚è³‡æ–™

    for city_id in sorted(all_ids):
        country, region, city = city_data[city_id][1], city_data[city_id][2], city_data[city_id][3]

        # å¦‚æœ `City == "-"`ï¼Œå‰‡å¡«ç©ºï¼Œä¸æŸ¥è©¢
        if city == "-":
            all_results.append([city_id, country, region, city, ""])
            continue  # è·³éæŸ¥è©¢

        if city_id in existing_data:
            # è‹¥å·²å­˜åœ¨ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æŸ¥è©¢
            row = existing_data[city_id]
            if row[4] == "N/A":  # è‹¥ä¸­æ–‡åç¨±æ˜¯ "N/A"ï¼Œé‡æ–°æŸ¥è©¢
                city_list.append((city_id, country, region, city))
            all_results.append(row)  # åŠ å…¥ç¾æœ‰è³‡æ–™
        else:
            # éœ€è¦æŸ¥è©¢çš„åŸå¸‚
            city_list.append((city_id, country, region, city))

    # **é™åˆ¶æ¯æ¬¡æŸ¥è©¢ 100 ç­†**
    if city_list:
        city_list = city_list[:BATCH_SIZE]  # åªå–å‰ 100 ç­†
        print(f"ğŸ” é€™æ¬¡åŸ·è¡Œæœ€å¤šæŸ¥è©¢ {len(city_list)} ç­†")
        results = await fetch_all_chinese_names(city_list)
        all_results.extend(results)  # åŠ å…¥æ–°çš„æŸ¥è©¢çµæœ
    else:
        print("âœ… æ‰€æœ‰åŸå¸‚çš„ä¸­æ–‡åç¨±éƒ½å·²æŸ¥è©¢å®Œæˆï¼Œç„¡éœ€ç¹¼çºŒã€‚")

    # **ç¢ºä¿ ID é‡æ–°æ’åº**
    all_results.sort(key=lambda x: int(x[0]))

    # å­˜å…¥ CSV
    with open(output_file, mode="w", newline="", encoding="utf-8") as outfile:
        writer = csv.writer(outfile)
        writer.writerow(["ID", "Country", "Region", "City", "Chinese Name"])  # å¯«å…¥æ¨™é¡Œ
        writer.writerows(all_results)

    print(f"âœ… ä¸­æ–‡åç¨±æŸ¥è©¢å®Œæˆï¼Œå­˜å…¥ {output_file}ï¼Œé€™æ¬¡åŸ·è¡ŒçµæŸï¼")

# åŸ·è¡Œ
asyncio.run(main())
